{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning how to classify digits using Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is hosted on Yann LeCun's website. If you are copying and pasting in the code from this tutorial, start here with these two lines of code which will download and read in the data automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is split into three parts: 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation). This split is very important: it's essential in machine learning that we have separate data which we don't learn from so that we can make sure that what we've learned actually generalizes!\n",
    "\n",
    "As mentioned earlier, every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We'll call the images \"x\" and the labels \"y\". Both the training set and test set contain images and their corresponding labels; for example the training images are mnist.train.images and the training labels are mnist.train.labels.\n",
    "\n",
    "Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with importing tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need a placeholder where we will feed in the data. Next, we will also need two varaibles: W and b for the weights and biases that we will be learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784], name=\"x\")\n",
    "W = tf.Variable(tf.zeros([784, 10]), name=\"W\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are building the computation graph of a single layer neural network, we can use the inbuilt functions that tensorflow has to offer to do the matrix multiplication and addition and then finally passing it through a softmax to get the probabilities of the input images belonging to various classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another placeholder `y_` which would contain the actual classes of the image that we have fed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name='y_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us use the cross entrophy loss to define how we are going to calculate the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we go ahead and define the trainingOp for our graph that will be responsible for optimizing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now officially done with implementing the computation graph. Let's go ahead and create a Tensorflow session that we will use to run our compute graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing that we want to do is to initialize the variables defined in the compute graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a writer that you can write to and use it for tensorboard. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('./graphs', sess.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train for 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate the error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.6599988937\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def RNN(x, seq_lens, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    #x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    cell = tf.contrib.rnn.GRUCell(n_hidden) \n",
    "\n",
    "    # Roll over data. Note that seq length isn't required here \n",
    "    outputs, last_state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32) \n",
    "    \n",
    "    # We won't have to do this for the HW, but since we want ONLY the last output, we can tf.gather it\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    last_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return last_output# tf.matmul(last_output, weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, seq_lens, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be same size: logits_size=[28,10] labels_size=[128,10]\n\t [[Node: SoftmaxCrossEntropyWithLogits_1 = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_3, Reshape_4)]]\n\nCaused by op u'SoftmaxCrossEntropyWithLogits_1', defined at:\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-54cd8af6d1ee>\", line 22, in <module>\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1646, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2305, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[28,10] labels_size=[128,10]\n\t [[Node: SoftmaxCrossEntropyWithLogits_1 = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_3, Reshape_4)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e887e189f8b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_seq_lens\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Calculate batch accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[28,10] labels_size=[128,10]\n\t [[Node: SoftmaxCrossEntropyWithLogits_1 = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_3, Reshape_4)]]\n\nCaused by op u'SoftmaxCrossEntropyWithLogits_1', defined at:\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/ajaysohmshetty/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-54cd8af6d1ee>\", line 22, in <module>\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1646, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2305, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/ajaysohmshetty/Developer/CS224S_TA/hw3/tensorflow_1_venv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[28,10] labels_size=[128,10]\n\t [[Node: SoftmaxCrossEntropyWithLogits_1 = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_3, Reshape_4)]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_seq_lens = [n_steps]*batch_size\n",
    "        \n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, seq_lens: batch_seq_lens})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for mnist test images\n",
    "    test_data = mnist.test.images.reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
